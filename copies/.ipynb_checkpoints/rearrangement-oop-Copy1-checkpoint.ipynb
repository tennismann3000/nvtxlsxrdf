{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rdflib\n",
    "import numpy as np\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal, RDF, util\n",
    "from rdflib.namespace import NamespaceManager\n",
    "import datetime\n",
    "pd.options.display.max_colwidth = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(df, mapping, origin):\n",
    "    ## noch ein bisschen sauberer wäre gut, zeilenumbruch nach beschreibung vllt und personennamen in richtiger schreibweise\n",
    "    ## bei result = texts müssten Autoren noch mit den Labels der Personen, auf die verwiesen wird, versehen werden\n",
    "    if origin == \"zeitraum\":\n",
    "        col1 = mapping[\"zeitraum1\"]\n",
    "        col2 = mapping[\"zeitraum2\"]\n",
    "        col3 = mapping[\"zeitraum\"]\n",
    "        col4 = mapping[\"zeitraum_type\"]\n",
    "        df.loc[df[col1].notna() & df[col2].notna(), col3] = \"timespan_\" + df[mapping[\"ID\"]] + \"_\" + df[col1].apply(str).str[:4] + \"_\" + df[col2].apply(str).str[:4]\n",
    "        df.loc[df[col1].notna() & df[col2].isna(), col3] = \"timespan_\" + df[mapping[\"ID\"]] + \"_\" + df[col1].apply(str).str[:4]\n",
    "        df.loc[df[col1].isna() & df[col2].notna(), col3] = \"timespan_\" + df[mapping[\"ID\"]] + \"_\" + df[col2].apply(str).str[:4]\n",
    "        df[col4] = df[col3]\n",
    "    \n",
    "    if origin == \"season\":\n",
    "        col1 = mapping[\"season1\"]\n",
    "        col2 = mapping[\"season2\"]\n",
    "        col3 = mapping[\"season\"]\n",
    "        col4 = mapping[\"season_type\"]\n",
    "        df.loc[df[col1].notna() & df[col2].notna(), col3] = \"season_\" + df[mapping[\"ID\"]] + \"_\" + df[col1].apply(str).str[:4] + \"_\" + df[col2].apply(str).str[:4]\n",
    "        df.loc[df[col1].notna() & df[col2].isna(), col3] = \"season_\" + df[mapping[\"ID\"]] + \"_\" + df[col1].apply(str).str[:4]\n",
    "        df.loc[df[col1].isna() & df[col2].notna(), col3] = \"season_\" + df[mapping[\"ID\"]] + \"_\" + df[col2].apply(str).str[:4]\n",
    "        df[col4] = df[col3]\n",
    "  \n",
    "    if origin == \"description\":\n",
    "        col1 = mapping[\"description1\"]\n",
    "        col2 = mapping[\"description2\"]\n",
    "        col3 = mapping[\"description\"]\n",
    "        df.loc[df[col1].notna() & df[col2].notna(), col3] = df[col1] + \" Quelle: \" + df[col2]\n",
    "        df.loc[df[col1].notna() & df[col2].isna(), col3] = df[col1]\n",
    "        df.loc[df[col1].isna() & df[col2].notna(), col3] = \"Quelle: \" + df[col2]        \n",
    "    \n",
    "    if origin == \"texts\":\n",
    "        col1 = mapping[\"texts1\"]\n",
    "        col2 = mapping[\"texts2\"]\n",
    "        col3 = mapping[\"texts\"]\n",
    "        df[col2] = peoplereplace(df[col2])\n",
    "        df.loc[df[col1].notna() & df[col2].notna(), col3] = \"Text(e): \" + df[col1] + \" Autor(en): \" + df[col2]\n",
    "        df.loc[df[col1].notna() & df[col2].isna(), col3] = \"Text(e): \" + df[col1]\n",
    "        df.loc[df[col1].isna() & df[col2].notna(), col3] = \"Autor(en): \" + df[col2]\n",
    "        \n",
    "\n",
    "    if origin == \"condition\":\n",
    "        col1 = mapping[\"condition1\"]\n",
    "        col2 = mapping[\"condition2\"]\n",
    "        col3 = mapping[\"condition\"]\n",
    "        df.loc[df[col1].notna() & df[col2].notna(), col3] = \"Zustand: \" + df[col1] + \" Datum Zustandsaufnahme: \" + df[col2].apply(str)\n",
    "        df.loc[df[col1].notna() & df[col2].isna(), col3] = \"Zustand: \" + df[col1]\n",
    "        df.loc[df[col1].isna() & df[col2].notna(), col3] = \" Datum Zustandsaufnahme: \" + df[col2].apply(str)       \n",
    "\n",
    "    if origin == \"institution\":\n",
    "        col1 = mapping[\"ID\"]\n",
    "        col2 = mapping[\"institution1\"]\n",
    "        col3 = mapping[\"institution\"]\n",
    "        df.loc[df[col2].notna(), col3] = df[col1].str.replace(\"LOC_\", \"G_\")\n",
    "        \n",
    "    if origin == \"physical\": # neue if-Kondition für objekte, die physisch nicht exisitieren\n",
    "        col1 = mapping[\"ID\"]\n",
    "        col3 = mapping[\"physical\"]\n",
    "        col4 = mapping[\"phys_type\"]\n",
    "        df.loc[df[col1].notna(), col3] = \"phys_\" + df[col1]\n",
    "        df[col4] = df[col3]\n",
    "        \n",
    "    if origin == \"digital\": # neue if-Kondition für objekte, die digital nicht exisitieren\n",
    "        col1 = mapping[\"ID\"]\n",
    "        col3 = mapping[\"digital\"]\n",
    "        col4 = mapping[\"digi_type\"]\n",
    "        df.loc[df[col1].notna(), col3] = \"digi_\" + df[col1]\n",
    "        df[col4] = df[col3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_reindex(df, index=None):\n",
    "    df = (df.sort_values(by=[index])\n",
    "          .reset_index(drop=True) ## resets index to be continouus            \n",
    "         )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refreplace(col):\n",
    "    ## string converter for URIs\n",
    "    col = col.str.lower()\n",
    "    col = col.str.replace(\"ä\", \"ae\")\n",
    "    col = col.str.replace(\"ö\",\"oe\")\n",
    "    col = col.str.replace(\"ü\",\"ue\")\n",
    "    col = col.str.replace(\"ß\",\"ss\")\n",
    "    col = col.str.replace(\"!?,\\.\", \"\")\n",
    "    col = col.str.replace(\"[^a-zA-Z0-9;]\", \"_\")\n",
    "    col = col.str.replace(\"-\", \"_\")\n",
    "    col = col.str.replace(\"_{2,}\", \"_\")\n",
    "    col = col.str.strip(\"_\")\n",
    "    col = col.str.strip()\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supersplit2(col, meta=False):\n",
    "    ## for more convenient str splitting within the columns (expand=false), mainly for splitting all URIs\n",
    "    if meta==True:\n",
    "        col = col.str.replace(\":\",\".\")\n",
    "        col.str.split(\",\", expand = False)\n",
    "    if col.any():\n",
    "        col = col.str.split(\";\", expand = False)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supersplit(col):\n",
    "    ## for more convenient str splitting within the columns (expand=false), mainly for splitting all URIs\n",
    "    if col.any():\n",
    "        col = col.str.split(\";\", expand = False)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitreplace(col):\n",
    "    ## combines refreplace and supersplit in correct order for they are both always used for any columns containing URIs\n",
    "    col = refreplace(col)\n",
    "    col = supersplit(col)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peoplereplace(df):\n",
    "    ## temporary string beautifier for persons as literal values\n",
    "    df = df.str.replace(\"_\",\" \")\n",
    "    df = df.str.replace(\",\", \" \")\n",
    "    df = df.str.replace(\";\", \"; \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_rdf(column, nsm):\n",
    "    newlist = []\n",
    "    for i in column:\n",
    "        if isinstance(i, list):\n",
    "            newlistlist = []\n",
    "            for j in i:\n",
    "                newlistlist.append(util.from_n3(j, nsm=nsm))\n",
    "            newlist.append(newlistlist)\n",
    "        else:\n",
    "            newlist.append(float(\"NaN\"))\n",
    "    column = pd.Series(newlist)\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = Namespace(\"http://lod.iti-germany.de/contexts/\")\n",
    "dc = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")\n",
    "edm = Namespace(\"http://www.europeana.eu/schemas/edm/\")\n",
    "foaf = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "mtt_a = Namespace(\"http://lod.iti-germany.de/schema/mtt_a/\")\n",
    "nvt = Namespace(\"http://lod.iti-germany.de/resource/\")\n",
    "nvto = Namespace(\"http://lod.iti-germany.de/schema/nvto/\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rdf = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "rdfs = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "wgs84_pos = Namespace(\"http://www.w3.org/2003/01/geo/wgs84_pos#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsm = NamespaceManager(Graph())\n",
    "nsm.bind(\"context\", \"http://lod.iti-germany.de/contexts/\")\n",
    "nsm.bind(\"dc\", \"http://purl.org/dc/elements/1.1/\")\n",
    "nsm.bind(\"dcterms\", \"http://purl.org/dc/terms/\")\n",
    "nsm.bind(\"edm\", \"http://www.europeana.eu/schemas/edm/\")\n",
    "nsm.bind(\"foaf\", \"http://xmlns.com/foaf/0.1/\")\n",
    "nsm.bind(\"mtt_a\", \"http://lod.iti-germany.de/schema/mtt_a/\")\n",
    "nsm.bind(\"nvt\", \"http://lod.iti-germany.de/resource/\")\n",
    "nsm.bind(\"nvto\", \"http://lod.iti-germany.de/schema/nvto/\")\n",
    "nsm.bind(\"owl\", \"http://www.w3.org/2002/07/owl#\")\n",
    "nsm.bind(\"rdf\", \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "nsm.bind(\"rdfs\", \"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "nsm.bind(\"skos\", \"http://www.w3.org/2004/02/skos/core#\")\n",
    "nsm.bind(\"wgs84_pos\", \"http://www.w3.org/2003/01/geo/wgs84_pos#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bindbind(graph):\n",
    "    graph.bind(\"context\", \"http://lod.iti-germany.de/contexts/\")\n",
    "    graph.bind(\"dc\", \"http://purl.org/dc/elements/1.1/\")\n",
    "    graph.bind(\"dcterms\", \"http://purl.org/dc/terms/\")\n",
    "    graph.bind(\"edm\", \"http://www.europeana.eu/schemas/edm/\")\n",
    "    graph.bind(\"foaf\", \"http://xmlns.com/foaf/0.1/\")\n",
    "    graph.bind(\"mtt_a\", \"http://lod.iti-germany.de/schema/mtt_a/\")\n",
    "    graph.bind(\"nvt\", \"http://lod.iti-germany.de/resource/\")\n",
    "    graph.bind(\"nvto\", \"http://lod.iti-germany.de/schema/nvto/\")\n",
    "    graph.bind(\"owl\", \"http://www.w3.org/2002/07/owl#\")\n",
    "    graph.bind(\"rdf\", \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "    graph.bind(\"rdfs\", \"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "    graph.bind(\"skos\", \"http://www.w3.org/2004/02/skos/core#\")\n",
    "    graph.bind(\"wgs84_pos\", \"http://www.w3.org/2003/01/geo/wgs84_pos#\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvt_ds = rdflib.Dataset()\n",
    "nvt_ds = bindbind(nvt_ds)\n",
    "pr_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/productions\")\n",
    "ev_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/events\")\n",
    "vid_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/videos\")\n",
    "text_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/texts\")\n",
    "img_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/images\")\n",
    "aud_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/audio\")\n",
    "person_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/persons\")\n",
    "group_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/groups\")\n",
    "loc_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/locations\")\n",
    "city_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/cities\")\n",
    "country_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/countries\")\n",
    "col_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/collections\")\n",
    "series_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/series\")\n",
    "concept_graph = nvt_ds.graph(identifier=\"http://lod.iti-germany.de/contexts/concepts\")\n",
    "graph_list = [pr_graph, ev_graph, vid_graph, text_graph, img_graph, aud_graph, person_graph, group_graph, loc_graph, city_graph, country_graph, col_graph, series_graph, concept_graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvt_master = pd.read_excel('./NVT_Metadatentabelle_MASTER_0609.xlsm', sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapped = pd.read_excel('./Mapping_Meta_Py.xlsx', sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sheet():\n",
    "## name: Name des Reiters\n",
    "## head: Indikator, wo im df Tabellenüberschriften zu finden sind (0 = Inhalte Zeile 1, 1 = Inhalte Zeile 2, x = Index unverändert)\n",
    "## df: DataFrame pro Sheet\n",
    "## tdf: Kopie von df, mit über head definierter Indexanpassung\n",
    "## tdf_empty_transposed: DataFrame, der nur aus Überschriften (ohne Spaltenachsennamen) besteht, zur Konvertierung nach Excel, inklusive Transponierung\n",
    "    def __init__(self, name, head, df, graph):\n",
    "        self.name = name\n",
    "        self.head = head\n",
    "        self.df = df\n",
    "        self.graph = graph\n",
    "        self.tdf = df.copy()\n",
    "        self.column_names = None\n",
    "        self.tdf_empty_transposed = None\n",
    "        \n",
    "        if self.head != \"x\":\n",
    "            self.tdf.columns = self.tdf.iloc[self.head]\n",
    "            self.tdf.columns.name = None\n",
    "        else:\n",
    "            pass\n",
    "        if isinstance(self.tdf, pd.DataFrame):\n",
    "            self.column_names = self.tdf.columns.tolist()\n",
    "            self.tdf_empty_transposed = self.tdf[0:0].transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zur Erzeungung von Sheet Instanzen mit Name und Spaltenkopfreferenz als Liste und mit dict Referenz\n",
    "## column_headers: Liste von 'echten' Spaltenüberschriften, 0, 1, für erste, zweite Zeile, x=Standard\n",
    "## col_ref dict um nicht mit Zahlen, sondern Namen zu arbeiten\n",
    "## sheet_dict manuell, falls Verschiebungen in Tabelle passieren\n",
    "sheet_list = []\n",
    "col_ref = {}\n",
    "sheet_dict = {\"Produktionen\":0,\n",
    "             \"Ereignisse\":0,\n",
    "             \"Objekte VIDEOS\":0,\n",
    "             \"Objekte TEXT\":0,\n",
    "             \"Objekte BILD\":0,\n",
    "             \"Objekte AUDIO\":0,\n",
    "             \"||_Personen\":1,\n",
    "             \"||_Gruppen_Ensembles\":\"x\",\n",
    "             \"||_Veranstaltungsort\":\"x\",\n",
    "             \"||_Städte\":\"x\",\n",
    "             \"||_Länder\":\"x\",\n",
    "             \"Sammlungen\":\"x\",\n",
    "             \"Serie\":\"x\"}\n",
    "\n",
    "for idx, i in enumerate(sheet_dict.keys()):\n",
    "    sheet_list.append(Sheet(name = i, head = sheet_dict[i], df = nvt_master[i], graph = graph_list[idx]))\n",
    "    col_ref[i] = sheet_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping():\n",
    "    \n",
    "    def __init__(self, sheet, df, title, rdfp, original, subj, uritype, objtype, merge, titledict, sheetid):\n",
    "        self.sheet = sheet\n",
    "        self.df = df\n",
    "        self.title = title\n",
    "        self.rdfp = rdfp\n",
    "        self.original = original\n",
    "        self.subj = subj\n",
    "        self.uritype = uritype\n",
    "        self.objtype = objtype\n",
    "        self.merge = merge\n",
    "        self.titledict = titledict\n",
    "        self.sheetid = sheetid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_list = []\n",
    "merger = pd.DataFrame\n",
    "titlekey = pd.DataFrame\n",
    "for key,value in columns_mapped.items():\n",
    "    # value.dropna(subset=[\"NVTO Property\"], inplace=True) # droppt alle Spaltennamen Einträge ohne korrespondierende NVTO Property\n",
    "    \n",
    "    value[['URI Type','NVTO Property']] = value[['URI Type','NVTO Property']].apply(supersplit, axis=1) # splittet alle Spalten mit RDF-Properties oder Classes\n",
    "    \n",
    "    value[\"NVTO Property\"] = str_to_rdf(value[\"NVTO Property\"], nsm) # wandelt die Werte der Spalte in rdflib URIRefs um\n",
    "    \n",
    "    value[\"URI Type\"] = str_to_rdf(value[\"URI Type\"], nsm) # wandelt die Werte der Spalte in rdflib URIRefs um\n",
    "    \n",
    "    merge = (value[['Spaltenname in Tabelle','Merge']].dropna(subset=[\"Merge\"])\n",
    "                                                    .set_index('Merge').T.to_dict('records')) # merge baut dict zum Mergen von \n",
    "    \n",
    "    titledict = value.set_index('Spaltenname in Tabelle').T.to_dict('list')\n",
    "    sheetid = value.loc[value['Merge']==\"ID\", 'Spaltenname in Tabelle'].iloc[0]\n",
    "    mapping_list.append(Mapping(sheet = key, #baut Klassenobjekte mit den unten genannten Einträgen\n",
    "                                df = value, \n",
    "                                title = value[\"Spaltenname in Tabelle\"], \n",
    "                                rdfp = value[\"NVTO Property\"],\n",
    "                                original = value[\"Merge\"],\n",
    "                                subj = value[\"Subject\"],\n",
    "                                uritype = value[\"URI Type\"],\n",
    "                                objtype = value[\"Object Type\"],\n",
    "                                merge = merge,\n",
    "                                titledict = titledict,\n",
    "                                sheetid = sheetid\n",
    "                                ) \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(elem in list(mapping_list[0].title)  for elem in sheet_list[0].column_names) ## gegencheck, ist mapping_meta_py ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## erweitern, um nicht nur zu writen, sondern zukünftig auch upzudaten, und wenn update dann ausgeben: bitte nachtragen, also ein \"erwriterer\"\n",
    "\n",
    "## writer = pd.ExcelWriter('Columns_for_Mapping.xlsx', engine='xlsxwriter')\n",
    "## for i in sheet_list:\n",
    "##     i.tdf_empty_transposed.to_excel(writer, header=False, sheet_name=i.name)\n",
    "## writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bei Produktionen sollte am Ende nach 'XX_' gesucht werden, nicht nach PR, und der Typ bei Produktionen wird dann durch die Typenspalte bestimmt\n",
    "tdf = col_ref[\"Produktionen\"].tdf\n",
    "tdf = (tdf[tdf['Identifier / geeinigter Name'].str.contains('PR', na=False)] ## drops all rows that don't start with 'PR' or are NaN\n",
    "          .drop(tdf[tdf['Identifier / geeinigter Name'].str.contains('PR_Internationaler_Workshop_zur_Biomechanik_GITIS Moskau_Januar_1993', na=False)].index)\n",
    "       )\n",
    "col_ref[\"Produktionen\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Ereignisse\"].tdf\n",
    "tdf = (tdf[tdf['Identifier / geeinigter Name'].str.contains('EV', na=False)] ## drops all rows that don't start with 'PR' or are NaN\n",
    "          .drop(tdf[tdf['Identifier / geeinigter Name'].str.contains('EV_Internationaler_Workshop_zur_Biomechanik_GITIS Moskau_Januar_1993_001', na=False)].index) ## drops example line\n",
    "      )\n",
    "col_ref[\"Ereignisse\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Objekte VIDEOS\"].tdf\n",
    "tdf = tdf[tdf['Projekt ID'].str.contains('vid', na=False)]\n",
    "col_ref[\"Objekte VIDEOS\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Objekte TEXT\"].tdf\n",
    "tdf = tdf[tdf['Projekt ID'].str.contains('txt', na=False)]\n",
    "col_ref[\"Objekte TEXT\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Objekte BILD\"].tdf\n",
    "tdf = tdf[tdf['Projekt ID'].str.contains('img', na=False)]\n",
    "col_ref[\"Objekte BILD\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Objekte AUDIO\"].tdf\n",
    "tdf = tdf[tdf['Projekt ID'].str.contains('aud', na=False)]\n",
    "col_ref[\"Objekte AUDIO\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"||_Personen\"].tdf\n",
    "tdf = tdf.drop(tdf[tdf['Identifier / geeinigte Schreibweise'].str.contains('Vorname Vatersname,Nachname', na=False)].index)\n",
    "col_ref[\"||_Personen\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"||_Gruppen_Ensembles\"].tdf\n",
    "tdf = tdf[tdf['Gruppe Identifier / geeinigte Schreibweise'].str.contains('G_', na=False)]\n",
    "col_ref[\"||_Gruppen_Ensembles\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"||_Veranstaltungsort\"].tdf\n",
    "tdf = tdf[tdf['PROJEKT'].str.contains('LOC_', na=False)]\n",
    "col_ref[\"||_Veranstaltungsort\"].tdf = tdf\n",
    "       \n",
    "tdf = col_ref[\"Sammlungen\"].tdf\n",
    "tdf = (tdf[tdf['PROJEKT'].str.contains('COL_', na=False)]\n",
    "          .drop(tdf[tdf['PROJEKT'].str.contains('COL_Identifier', na=False)].index)\n",
    "      )\n",
    "col_ref[\"Sammlungen\"].tdf = tdf\n",
    "\n",
    "tdf = col_ref[\"Serie\"].tdf      \n",
    "tdf = (tdf[tdf['PROJEKT'].str.contains('SRS_', na=False)]\n",
    "           .drop(tdf[tdf['PROJEKT'].str.contains('SRS_Identifier', na=False)].index)\n",
    "       )\n",
    "col_ref[\"Serie\"].tdf = tdf\n",
    "\n",
    "for sdx, sheet in enumerate(col_ref):\n",
    "      col_ref[sheet].tdf = sort_reindex(col_ref[sheet].tdf, index=mapping_list[sdx].sheetid) ## das könnte auch für alle auf einmal passieren for tdf sort_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uri_list(column2, predicate, column=\"ID\"):\n",
    "    if isinstance(row[column], list):\n",
    "        idid = row[column][0] ## weil id immer einzigartig ist\n",
    "        if isinstance(row[column2], list):\n",
    "            for i in row[column2]:\n",
    "                graph.add((nvt[idid], predicate, nvt[str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für alle Spalten in allen Sheets wird merge_columns angewandt\n",
    "# mapping_list[sdx].title = alle Spalten\n",
    "# mapping_list[sdx].merge[0] = ID des Sheets, 0, weil Dict seltsam in Liste\n",
    "# mapping_list[sdx].original[cdx] = für die Keywords in merge_columns, könnte man vllt auch mit merge[0] machen\n",
    "\n",
    "for sdx, sheet in enumerate(sheet_list):\n",
    "    for cdx, column in enumerate(mapping_list[sdx].title):\n",
    "        merge_columns(sheet.tdf, mapping_list[sdx].merge[0], mapping_list[sdx].original[cdx]) # für die zusammengesetzten Spalten für z.B. Spielzeit Beschreibung Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sheet_list[10].tdf[\"geonames URI\"].value_counts()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schleife muss getrennt von der oberen laufen, weil in der unteren Listen erzeugt werden, die nicht mehr für die oberen verwendbar sind\n",
    "# mapping_list[sdx].title = alle Spalten\n",
    "# mapping_list[sdx].titledict[column][5] verbindet Spalte mit Keyword \"Reference\"\n",
    "# splitreplace wandelt alle Reference Zeilen zu Listen mit Referenzen um\n",
    "\n",
    "for sdx, sheet in enumerate(sheet_list):\n",
    "    for cdx, column in enumerate(mapping_list[sdx].title):\n",
    "        reference = mapping_list[sdx].titledict[column][5]\n",
    "        if reference == \"Reference\" and len(sheet_list[sdx].tdf[column].value_counts()) > 0:\n",
    "            sheet_list[sdx].tdf[column] = splitreplace(sheet_list[sdx].tdf[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sheet_list[0].tdf.columns) == list(mapping_list[0].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " nan,\n",
       " nan,\n",
       " [rdflib.term.URIRef('http://lod.iti-germany.de/schema/nvto/hasSeason')],\n",
       " 'has season',\n",
       " 'Reference',\n",
       " nan]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_list[0].titledict[\"Spielzeit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prod -> sheet.tdf in sheet_list\n",
    "## mapping -> mapping_list[sdx]\n",
    "## for sdx, sheet in enumerate(sheet_list):\n",
    "## for rdx, row in sheet_list[sdx].tdf.iterrows\n",
    "for sdx, sheet in enumerate(sheet_list):\n",
    "    \n",
    "    tdf = sheet.tdf\n",
    "    mapping = mapping_list[sdx]\n",
    "    graph = sheet.graph\n",
    "\n",
    "    for rdx, row in tdf.iterrows(): # Iteration durch die Zeilen eines Sheets, row ist eine ganze Zeile \n",
    "        for cdx, column in enumerate(row): # pro Spalte pro Zeile, column ist der Wert einer Spalte pro Zeile\n",
    "\n",
    "            #Setup der Variablen\n",
    "            rdfsubj = mapping.titledict[tdf.columns[cdx]][1]\n",
    "            rdftype = mapping.titledict[tdf.columns[cdx]][2]\n",
    "            rdfprop = mapping.titledict[tdf.columns[cdx]][3]\n",
    "            objtype = mapping.titledict[tdf.columns[cdx]][5]\n",
    "\n",
    "            if tdf.columns[cdx] in list(mapping.title) and isinstance(rdfprop, list): # alle rdfproperties sind in Listen gespeichert, wenn nicht Liste dann NaN\n",
    "\n",
    "                for pdx, prop in enumerate(rdfprop): # pro Property der Spalte\n",
    "\n",
    "                    if isinstance(rdfsubj, str) and (isinstance(column, str) or isinstance(column, list)): # nur wenn ein anderes Subjekt in Meta_Py angegeben wird, wird idid angepasst\n",
    "                        idid = tdf[rdfsubj][rdx][0]\n",
    "                        #print(cdx, rdfsubj, column)\n",
    "                    else:\n",
    "                        idid = tdf[mapping.sheetid][rdx][0]\n",
    "\n",
    "                    if isinstance(column, str): #str sind nicht als Liste gespeichert\n",
    "                        #print(\"literal:\", cdx, idid, prop, str(column))\n",
    "                        if isinstance(objtype, str) and \"language\" in objtype:\n",
    "                            language = objtype\n",
    "                        else:\n",
    "                            language = None  \n",
    "                        graph.add((nvt[idid], prop, Literal(column, lang=language)))\n",
    "\n",
    "                    if isinstance(column, list) and not isinstance(rdftype, list): # References sind als Listen gespeichert\n",
    "                        for rdfobj in column:\n",
    "                            if idid != rdfobj:\n",
    "                                #print(\"all:\", cdx, idid, prop, rdfobj)\n",
    "                                graph.add((nvt[idid], prop, nvt[rdfobj]))\n",
    "\n",
    "\n",
    "                    if isinstance(rdftype, list) and isinstance(column, list): # für zusätzliche Klassenzugehörigkeit (Bourgeoisie, Proletariat)\n",
    "                        subject = column[0] # dem Wert der Spalte wird eine Klasse zugewiesen\n",
    "                        for irdftype in rdftype:\n",
    "                            #print(\"type:\", cdx, subject, prop, irdftype)\n",
    "                            graph.add((nvt[subject], prop, irdftype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nvt_ds_2.trig', 'wb') as f_trig:\n",
    "    f_trig.write(nvt_ds.serialize(format=\"trig\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "colcol = [\"A\", \"B\", \"C\"]\n",
    "rowrow = [\"D\", \"E\", \"F\"]\n",
    "datedate = np.array([[\"1 \", \"asdasd2     \", \"    2\"], [\"asdasd2     \", \"1 \", \"    2\"],[ \"    2\", \"1 \", \"asdasd2     \"]])\n",
    "testdf = pd.DataFrame(data=datedate, index=rowrow, columns=colcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>1</td>\n",
       "      <td>asdasd2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>asdasd2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdasd2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         A        B        C\n",
       "D        1  asdasd2        2\n",
       "E  asdasd2        1        2\n",
       "F        2        1  asdasd2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.applymap(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
